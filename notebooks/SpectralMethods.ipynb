{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b9f1040-1929-46c3-8228-b45e28238b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np \n",
    "import networkx as nx \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from scipy.sparse import csr_matrix\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm, eigvalsh\n",
    "\n",
    "def sbm_gender_homophily_adj_and_metadata(n_g1, n_g2, p_in, p_out, seed):\n",
    "    np.random.seed(seed)\n",
    "    sizes = [n_g1, n_g2]\n",
    "    probs = [[p_in, p_out], [p_out, p_in]]\n",
    "    G = nx.stochastic_block_model(sizes, probs, seed=seed)\n",
    "    adj_matrix = nx.to_numpy_array(G)\n",
    "    metadata = np.zeros((n_g1 + n_g2, 7))\n",
    "    metadata[:n_g1, 1] = 1  # First block: gender 1\n",
    "    metadata[n_g1:, 1] = 2  # Second block: gender 2\n",
    "    return adj_matrix, metadata\n",
    "\n",
    "def sbm_gender_homophily_adj_and_metadata(n_g1, n_g2, p_in, p_out, seed):\n",
    "    np.random.seed(seed)\n",
    "    sizes = [n_g1, n_g2]\n",
    "    probs = [[p_in, p_out], [p_out, p_in]]\n",
    "    G = nx.stochastic_block_model(sizes, probs, seed=seed)\n",
    "    adj_matrix = nx.to_numpy_array(G)\n",
    "    metadata = np.zeros((n_g1 + n_g2, 7))\n",
    "    metadata[:n_g1, 1] = 1  # First block: gender 1\n",
    "    metadata[n_g1:, 1] = 2  # Second block: gender 2\n",
    "    return adj_matrix, metadata\n",
    "\n",
    "def create_iid_core_fringe_graph(adj_matrix, k, seed=None, ff=False):\n",
    "    \"\"\"\n",
    "    Creates a core-fringe graph using an IID sample of size k as the core.\n",
    "    The fringe consists of all nodes connected to the core.\n",
    "    \n",
    "    Parameters:\n",
    "      - adj_matrix (scipy.sparse matrix): full graph adjacency matrix.\n",
    "      - k (int): number of core nodes to sample.\n",
    "      - seed (int, optional): random seed.\n",
    "      \n",
    "    Returns:\n",
    "      (core_fringe_adj, core_indices, fringe_indices)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    core_indices = np.random.choice(adj_matrix.shape[0], size=k, replace=False)\n",
    "    is_core = np.zeros(adj_matrix.shape[0], dtype=bool)\n",
    "    is_core[core_indices] = True\n",
    "\n",
    "    A = csr_matrix(adj_matrix)\n",
    "    total_edges_original = int(A.nnz / 2)\n",
    "    print(f\"Total edges in original adjacency: {total_edges_original}\")\n",
    "    neighbors = A[core_indices].nonzero()[1]\n",
    "    fringe_indices = np.setdiff1d(np.unique(neighbors), core_indices)\n",
    "    \n",
    "    fringe_frignge_adj = A[fringe_indices, :][:, fringe_indices]\n",
    "    # Build a mask that keeps only core-core and core-fringe edges.\n",
    "    # Create sparse mask matrix\n",
    "    mask_data = []\n",
    "    mask_rows = []\n",
    "    mask_cols = []\n",
    "    \n",
    "    # Add core-core edges\n",
    "    core_core_edges = A[core_indices][:, core_indices]\n",
    "    for i, j in zip(*np.triu_indices_from(core_core_edges, k=1)):\n",
    "        mask_rows.extend([core_indices[i], core_indices[j]])\n",
    "        mask_cols.extend([core_indices[j], core_indices[i]])\n",
    "        mask_data.extend([1, 1])\n",
    "    \n",
    "    # Add core-fringe edges\n",
    "    core_fringe_edges = A[core_indices][:, fringe_indices]\n",
    "    for i, j in zip(core_fringe_edges.nonzero()[0], core_fringe_edges.nonzero()[1]):\n",
    "        mask_rows.extend([core_indices[i], fringe_indices[j]])\n",
    "        mask_cols.extend([fringe_indices[j], core_indices[i]])\n",
    "        mask_data.extend([1, 1])\n",
    "\n",
    "    # Create sparse mask matrix\n",
    "    mask = csr_matrix((mask_data, (mask_rows, mask_cols)), shape=A.shape)\n",
    "\n",
    "    core_fringe_adj = A.multiply(mask)\n",
    "\n",
    "    print(f\"IID core\")\n",
    "    print(f\"Core size: {len(core_indices)}\")\n",
    "    core_core_edges = int(np.sum(core_fringe_adj[core_indices][:, core_indices]) / 2)\n",
    "    core_fringe_edges = int(np.sum(core_fringe_adj[core_indices][:, fringe_indices]) / 2)\n",
    "    print(f\"Number of core-core edges: {core_core_edges}\")\n",
    "    print(f\"Number of core-fringe edges: {core_fringe_edges}\")\n",
    "    \n",
    "    # Calculate number of fringe-fringe edges lost\n",
    "    fringe_fringe_edges = total_edges_original - (core_core_edges + core_fringe_edges)\n",
    "    print(f\"Number of fringe-fringe edges (lost): {fringe_fringe_edges}\")\n",
    "\n",
    "    # After constructing core_fringe_adj\n",
    "    fringe_adj = core_fringe_adj[fringe_indices, :][:, fringe_indices]\n",
    "    assert fringe_adj.nnz == 0, \"Fringe-fringe edges exist in the core-fringe adjacency matrix!\"\n",
    "\n",
    "    if ff:\n",
    "        return core_fringe_adj, core_indices, fringe_indices, fringe_frignge_adj\n",
    "    else:\n",
    "        return core_fringe_adj, core_indices, fringe_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a29fd8-e4d5-4c41-8ebb-41405e8d89a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e0862d-f055-49e2-b623-598fa22c111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(A, u, v, method='eigenValue2'):\n",
    "    subA = np.zeros(A.shape)\n",
    "    subA[u, :] = A[u, :]  # Entire row u\n",
    "    subA[v, :] = A[v, :]  # Entire row v\n",
    "    subA[:, u] = A[:, u]  # Entire column u\n",
    "    subA[:, v] = A[:, v]  # Entire column v\n",
    "    # Laplacian\n",
    "    L = np.diag(subA.sum(axis=1)) - subA\n",
    "    eigvals = np.sort(eigvalsh(L))\n",
    "\n",
    "    if method == 'eigenValue2':\n",
    "        # Return the 2nd smallest eigenvalue (Fiedler value)\n",
    "        return eigvals[1] if len(eigvals) > 1 else 0.0\n",
    "    elif method == 'eigenValue3':\n",
    "        # Return the 3rd smallest eigenvalue\n",
    "        return eigvals[2] if len(eigvals) > 2 else 0.0\n",
    "    elif method == 'nonZeroEigen':\n",
    "        # Return the smallest positive eigenvalue\n",
    "        pos_eigvals = eigvals[eigvals > 1e-8]\n",
    "        return pos_eigvals[0] if len(pos_eigvals) > 0 else 0.0\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Feature method '{method}' not implemented.\")\n",
    "\n",
    "def ff_imputation_lr_model(adj_matrix, core_indices, fringe_indices, method='eigenValue2', threshold=0.1, lr_kwargs=None):\n",
    "    \"\"\"\n",
    "    Impute the FF block using a LR model trained on core-core and core-fringe edges, by using the feature determined by the \n",
    "    \"method\" param.\n",
    "    \"\"\"\n",
    "    if lr_kwargs is None:\n",
    "        lr_kwargs = {'solver': 'liblinear', 'max_iter': 1000}\n",
    "\n",
    "    A = adj_matrix.toarray() if hasattr(adj_matrix, 'toarray') else np.array(adj_matrix)\n",
    "    n = A.shape[0]\n",
    "    A_pred = A.copy()\n",
    "    added_edges = 0\n",
    "    L = len(fringe_indices)\n",
    "    # build the training set \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for u in core_indices:\n",
    "        for v in core_indices:\n",
    "            if u >= v:\n",
    "                continue\n",
    "            feature = extract_feature(A, u, v, method)\n",
    "            X_train.append([feature])\n",
    "            y_train.append(A[u,v])\n",
    "\n",
    "    for u in core_indices:\n",
    "        for v in fringe_indices:\n",
    "            feature = extract_feature(A, u, v, method)\n",
    "            X_train.append([feature])\n",
    "            y_train.append(A[u,v])\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    # train LR model \n",
    "    model = LogisticRegression(**lr_kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict fringe-fringe edges\n",
    "    for u in fringe_indices:\n",
    "        for v in fringe_indices:\n",
    "            if u >= v:\n",
    "                continue \n",
    "\n",
    "            feature = extract_feature(A, u, v, method)\n",
    "            prob = model.predict_proba([[feature]])[0,1]\n",
    "            if prob >= threshold:\n",
    "                A_pred[u,v] = 1\n",
    "                A_pred[v,u] = 1\n",
    "                added_edges += 1\n",
    "\n",
    "    print(f\"Possible FF Edges: {L * (L-1)} \\t Added Edges : {added_edges * 2}\")\n",
    "    return A_pred, added_edges\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84179aaa-1138-46b1-bb8a-9572d369585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_confidence_interval(y_true, y_scores, n_bootstraps=1000, random_seed=42):\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    bootstrapped_scores = []\n",
    "    for i in range(n_bootstraps):\n",
    "        indices = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # skip if only one class in the sample\n",
    "            continue\n",
    "        score = roc_auc_score(y_true[indices], y_scores[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "    sorted_scores = np.sort(bootstrapped_scores)\n",
    "    lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "    upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "    return lower, upper\n",
    "    \n",
    "def logistic_regression_model(\n",
    "    adj_matrix, core_indices, \n",
    "    fringe_indices, metadata, \n",
    "    feature='link', lr_kwargs=None, \n",
    "    seed=None, threshold=0.5,\n",
    "    ff_imputation='eigenValue2'\n",
    "):\n",
    "    gender = metadata[:, 1].astype(int)  # Convert to integer\n",
    "    dorm = metadata[:, 4]\n",
    "    added_edges = 0\n",
    "    if ff_imputation:\n",
    "        # impute the fringe-fringe edges \n",
    "        ff_imputed_adj_matrix, added_edges = ff_imputation_lr_model(adj_matrix, core_indices, fringe_indices, method=ff_imputation, threshold=threshold, lr_kwargs=lr_kwargs)\n",
    "    else:\n",
    "        ff_imputed_adj_matrix = adj_matrix\n",
    "    if feature == 'link':\n",
    "        X_train = ff_imputed_adj_matrix[core_indices, :]\n",
    "        y_train = gender[core_indices]\n",
    "        X_test = ff_imputed_adj_matrix[fringe_indices, :]\n",
    "    elif feature == 'triangles':\n",
    "        # @toDo: implement this\n",
    "        X_train = ff_imputed_adj_matrix[core_indices, :]\n",
    "        y_train = gender[core_indices]\n",
    "        X_test = ff_imputed_adj_matrix[fringe_indices, :]\n",
    "    elif feature == 'node2vec':\n",
    "        # @toDo: implement this\n",
    "        X_train = ff_imputed_adj_matrix[core_indices, :]\n",
    "        y_train = gender[core_indices]\n",
    "        X_test = ff_imputed_adj_matrix[fringe_indices, :]\n",
    "        \n",
    "    print(\"\\n Feature Space (Core-Fringe)\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    y_test = gender[fringe_indices]\n",
    "    unique_train_classes = np.unique(y_train)\n",
    "    print(f\"Unique training classes: {unique_train_classes}\")\n",
    "    model = LogisticRegression(**lr_kwargs, random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    beta = model.coef_.flatten()\n",
    "    # print(f\"\\nModel Analysis:\")\n",
    "    # print(f\"Number of non-zero coefficients: {np.count_nonzero(beta)}\")\n",
    "    # print(f\"Mean absolute coefficient: {np.mean(np.abs(beta)):.4f}\")\n",
    "    # print(f\"Max coefficient: {np.max(np.abs(beta)):.4f}\")\n",
    "    # print(f\"Min coefficient: {np.min(np.abs(beta)):.4f}\")\n",
    "    # print(f\"Max coefficient (No-Abs): {np.max(beta):.4f}\")\n",
    "    # print(f\"Min coefficient (No-Abs): {np.min(beta):.4f}\")\n",
    "\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_scores = model.predict_proba(X_test)\n",
    "    \n",
    "    # Verify class order and AUC calculation\n",
    "    print(\"\\nClass Order Verification:\")\n",
    "    print(f\"Model classes_: {model.classes_}\")  # Order of classes in the model\n",
    "    print(f\"Unique test classes: {np.unique(y_test)}\")  # Classes in test set\n",
    "    print(f\"Class distribution in test: {dict(Counter(y_test))}\")\n",
    "    print(f\"Prediction distribution: {dict(Counter(y_test_pred))}\")\n",
    "\n",
    "    # Calculate AUC for each class\n",
    "    for i, class_label in enumerate(model.classes_):\n",
    "        class_auc = roc_auc_score(y_test == class_label, y_test_scores[:, i])\n",
    "        print(f\"AUC for class {class_label}: {class_auc:.4f}\")\n",
    "    \n",
    "    # Use the correct class index for AUC\n",
    "    positive_class_idx = np.where(model.classes_ == 2)[0][0]  # Assuming 2 is our positive class\n",
    "    auc = roc_auc_score(y_test, y_test_scores[:, positive_class_idx])\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Compute AUC confidence interval\n",
    "    auc_lower, auc_upper = auc_confidence_interval(y_test, y_test_scores[:, positive_class_idx])\n",
    "    print(f\"AUC 95% CI: [{auc_lower:.3f}, {auc_upper:.3f}]\")\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test ROC AUC: {auc:.4f}\")\n",
    "    print(f\"Training class distribution: {dict(Counter(y_train))}\")\n",
    "    print(f\"Test class distribution: {dict(Counter(y_test))}\")\n",
    "\n",
    "    return beta, accuracy, auc, (auc_lower, auc_upper), added_edges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56e13dfe-882e-4ac3-9026-fecaabd127c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n",
      "(1000, 7)\n",
      "(1000,)\n",
      "Total edges in original adjacency: 62779\n",
      "IID core\n",
      "Core size: 300\n",
      "Number of core-core edges: 5632\n",
      "Number of core-fringe edges: 13129\n",
      "Number of fringe-fringe edges (lost): 44018\n",
      "(1000, 1000)\n",
      "300\n",
      "700\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(fringe_indices))\n\u001b[32m     10\u001b[39m lr_kwargs = {\u001b[33m'\u001b[39m\u001b[33mC\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m100\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msolver\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mliblinear\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmax_iter\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1000\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m _, acc, auc, auc_ci, added_edges = logistic_regression_model(adj_matrix, core_indices, fringe_indices, \n\u001b[32m     12\u001b[39m                                                 metadata, feature=\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m, lr_kwargs=lr_kwargs, \n\u001b[32m     13\u001b[39m                                                 seed=seed, threshold=\u001b[32m0.10\u001b[39m, ff_imputation=\u001b[33m'\u001b[39m\u001b[33meigenValue2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m L = \u001b[38;5;28mlen\u001b[39m(fringe_indices)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPossible FF Edges: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m(L-\u001b[32m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m Added Edges : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madded_edges\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mlogistic_regression_model\u001b[39m\u001b[34m(adj_matrix, core_indices, fringe_indices, metadata, feature, lr_kwargs, seed, threshold, ff_imputation)\u001b[39m\n\u001b[32m     25\u001b[39m added_edges = \u001b[32m0\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ff_imputation:\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# impute the fringe-fringe edges \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     ff_imputed_adj_matrix, added_edges = ff_imputation_lr_model(adj_matrix, core_indices, fringe_indices, method=ff_imputation, threshold=threshold, lr_kwargs=lr_kwargs)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m     ff_imputed_adj_matrix = adj_matrix\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mff_imputation_lr_model\u001b[39m\u001b[34m(adj_matrix, core_indices, fringe_indices, method, threshold, lr_kwargs)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m u >= v:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m feature = extract_feature(A, u, v, method)\n\u001b[32m     46\u001b[39m X_train.append([feature])\n\u001b[32m     47\u001b[39m y_train.append(A[u,v])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mextract_feature\u001b[39m\u001b[34m(A, u, v, method)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Laplacian\u001b[39;00m\n\u001b[32m      8\u001b[39m L = np.diag(subA.sum(axis=\u001b[32m1\u001b[39m)) - subA\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m eigvals = np.sort(eigvalsh(L))\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m'\u001b[39m\u001b[33meigenValue2\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Return the 2nd smallest eigenvalue (Fiedler value)\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m eigvals[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eigvals) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/link/lib/python3.12/site-packages/numpy/linalg/linalg.py:1181\u001b[39m, in \u001b[36meigvalsh\u001b[39m\u001b[34m(a, UPLO)\u001b[39m\n\u001b[32m   1179\u001b[39m t, result_t = _commonType(a)\n\u001b[32m   1180\u001b[39m signature = \u001b[33m'\u001b[39m\u001b[33mD->d\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33md->d\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m w = gufunc(a, signature=signature, extobj=extobj)\n\u001b[32m   1182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m w.astype(_realType(result_t), copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "seed = random.seed(datetime.now().timestamp())\n",
    "sbm_adj_matrix, metadata = sbm_gender_homophily_adj_and_metadata(500, 500, 0.15, 0.1, seed=seed)\n",
    "print(sbm_adj_matrix.shape)\n",
    "print(metadata.shape)\n",
    "print(metadata[:, 1].shape)\n",
    "adj_matrix, core_indices, fringe_indices, ff_true = create_iid_core_fringe_graph(sbm_adj_matrix, 300, seed=seed, ff=True)\n",
    "print(adj_matrix.shape)\n",
    "print(len(core_indices))\n",
    "print(len(fringe_indices))\n",
    "lr_kwargs = {'C': 100, 'solver': 'liblinear', 'max_iter': 1000}\n",
    "_, acc, auc, auc_ci, added_edges = logistic_regression_model(adj_matrix, core_indices, fringe_indices, \n",
    "                                                metadata, feature='link', lr_kwargs=lr_kwargs, \n",
    "                                                seed=seed, threshold=0.10, ff_imputation='eigenValue2')\n",
    "L = len(fringe_indices)\n",
    "print(f\"Possible FF Edges: {L * (L-1)} \\t Added Edges : {added_edges * 2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
